{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf194e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1daf4b97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k9/2x8pccyj4l186_tbnbtkf8_40000gn/T/ipykernel_81164/3457405256.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     df_scaled = pd.DataFrame(scaled_data,\n\u001b[1;32m     19\u001b[0m                              \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                              \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1833\u001b[0m                         \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m                         \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m                     \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m                 \u001b[0;31m# try again, with .apply acting as a filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0;31m# operation, by excluding the grouping column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mSeries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \"\"\"\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_groupwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_indexed_same\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m             \u001b[0mnot_indexed_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0mresult_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k9/2x8pccyj4l186_tbnbtkf8_40000gn/T/ipykernel_81164/3457405256.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgroup_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mscaled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     df_scaled = pd.DataFrame(scaled_data,\n\u001b[1;32m     19\u001b[0m                              \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                              \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "seq_len = 10     # N days of history to look back\n",
    "target_var = 'Close'  # The price to predict (next day's close)\n",
    "ticker = 'Name'          # The column containing the ticker\n",
    "feature_list = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "batch_size = 64\n",
    "\n",
    "df = pd.read_csv(\"all_stocks_2017-01-01_to_2018-01-01.csv\")\n",
    "df = df.ffill()\n",
    "\n",
    "# ticker mapping\n",
    "ticker_list = df[ticker].unique()\n",
    "id_mapping = {id_val: idx for idx, id_val in enumerate(ticker_list)}\n",
    "\n",
    "# normalizing\n",
    "def group_scale(group):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(group[feature_list].value)\n",
    "    df_scaled = pd.DataFrame(scaled_data,\n",
    "                             columns= [col for col in feature_list],\n",
    "                             index = group.index)\n",
    "    \n",
    "df1 = df.groupby(ticker).apply(group_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdcb0dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7471,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 10     # N days of history to look back\n",
    "target_var = 'Close'  # The price to predict (next day's close)\n",
    "ticker = 'Name'          # The column containing the ticker\n",
    "feature_list = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def create_sequences_numpy(df, seq_len, target_var, feature_list, ticker, id_mapping):\n",
    "    X_sequences = []\n",
    "    y_targets = []\n",
    "    ticker_indices = []\n",
    "\n",
    "    # Process data \n",
    "    for ticker, group in df.groupby(ticker):\n",
    "        group = group.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "        ticker_idx = id_mapping[ticker]\n",
    "\n",
    "        features = group[feature_list].values\n",
    "        target = group[target_var].values\n",
    "\n",
    "        # Sliding Window Creation\n",
    "        for i in range(len(group) - seq_len):\n",
    "            # Input X: sequence of features for seq_len days\n",
    "            X = features[i : i + seq_len]\n",
    "            # Target Y: the target value immediately following the sequence\n",
    "            y = target[i + seq_len]\n",
    "\n",
    "            X_sequences.append(X)\n",
    "            y_targets.append(y)\n",
    "            ticker_indices.append(ticker_idx)\n",
    "\n",
    "    X_array = np.array(X_sequences)\n",
    "    y_array = np.array(y_targets)\n",
    "    ticker_array = np.array(ticker_indices, dtype=np.int64)\n",
    "\n",
    "    return X_array, y_array, ticker_array\n",
    "\n",
    "\n",
    "class StockTimeseriesDataset(Dataset):\n",
    "    def __init__(self, X_array, y_array, ticker_array):\n",
    "        # Store the NumPy arrays directly\n",
    "        self.X = X_array\n",
    "        self.y = y_array\n",
    "        self.ticker = ticker_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use torch.from_numpy() for zero-copy conversion to tensor\n",
    "        return torch.from_numpy(self.ticker[idx:idx+1]).squeeze(), \\\n",
    "               torch.from_numpy(self.X[idx]), \\\n",
    "               torch.from_numpy(self.y[idx:idx+1]).squeeze()\n",
    "\n",
    "\n",
    "# --- 3. EXECUTION PIPELINE ---\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "df = pd.read_csv(\"all_stocks_2017-01-01_to_2018-01-01.csv\")\n",
    "df = df.ffill()\n",
    "\n",
    "# Ticker Mapping\n",
    "ticker_list = df[ticker].unique()\n",
    "id_mapping = {id_val: idx for idx, id_val in enumerate(ticker_list)}\n",
    "\n",
    "# Normalization (Fit/Transform)\n",
    "scaler = MinMaxScaler()\n",
    "df[feature_list] = scaler.fit_transform(df[feature_list])\n",
    "\n",
    "\n",
    "X_array, y_array, ticker_array = create_sequences_numpy(\n",
    "    df, seq_len, target_var, feature_list, ticker, id_mapping\n",
    ")\n",
    "\n",
    "# Train/Test Split by ticker\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=123, stratify=df[ticker])\n",
    "\n",
    "# train_df.head()\n",
    "\n",
    "# --- Sequence Creation using the NumPy function ---\n",
    "\n",
    "# print(\"Starting NumPy sequence creation for Training data...\")\n",
    "# X_train, y_train, ticker_train = create_sequences_numpy(\n",
    "#     train_df, seq_len, target_var, features, ticker, id_mapping\n",
    "# )\n",
    "\n",
    "# print(\"Starting NumPy sequence creation for Testing data...\")\n",
    "# X_test, y_test, ticker_test = create_sequences_numpy(\n",
    "#     test_df, seq_len, target_var, features, ticker, id_mapping\n",
    "# )\n",
    "\n",
    "# print(f\"X_train (Samples, Timesteps, Features): {X_train.shape}\")\n",
    "# print(f\"y_train (Targets): {y_train.shape}\")\n",
    "# print(f\"ticker_train (IDs): {ticker_train.shape}\")\n",
    "\n",
    "\n",
    "# --- Dataset and DataLoader Creation ---\n",
    "\n",
    "# train_dataset = StockTimeseriesDataset(X_train, y_train, ticker_train)\n",
    "# test_dataset = StockTimeseriesDataset(X_test, y_test, ticker_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# print(f\"\\nDataLoaders created. Total training sequences: {len(train_dataset)}\")\n",
    "# print(f\"Total testing sequences: {len(test_dataset)}\")\n",
    "\n",
    "# # --- Verification ---\n",
    "# ticker_idx_batch, X_seq_batch, y_target_batch = next(iter(train_loader))\n",
    "\n",
    "# print(\"\\n--- Verification of PyTorch Batch Shapes (Final Output) ---\")\n",
    "# print(f\"Ticker ID Batch Shape: {ticker_idx_batch.shape} (Type: {ticker_idx_batch.dtype})\")\n",
    "# print(f\"X Sequence Batch Shape: {X_seq_batch.shape} (Type: {X_seq_batch.dtype})\")\n",
    "# print(f\"Y Target Batch Shape: {y_target_batch.shape} (Type: {y_target_batch.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3336d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATA PREPARATION CLASS\n",
    "class StockTimeseriesDataset(Dataset):\n",
    "    def __init__(self, df, seq_len, target_col, feature_cols, id_col, id_mapping):\n",
    "        self.seq_len = seq_len\n",
    "        self.X_sequences = []\n",
    "        self.y_targets = []\n",
    "\n",
    "        # Group data by Company ID (Name)\n",
    "        for ticker, group in df.groupby(id_col):\n",
    "            group = group.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "            # Get the Ticker index for embedding\n",
    "            ticker_idx = id_mapping[ticker]\n",
    "\n",
    "            # Extract features and target arrays\n",
    "            features = group[feature_cols].values\n",
    "            target = group[target_col].values\n",
    "\n",
    "            # Sliding Window Creation\n",
    "            for i in range(len(group) - seq_len):\n",
    "                X = features[i : i + seq_len]\n",
    "                y = target[i + seq_len]\n",
    "\n",
    "                self.X_sequences.append((ticker_idx, X))\n",
    "                self.y_targets.append(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the ticker index, the sequence tensor, and the target\n",
    "        ticker_idx, X_seq = self.X_sequences[idx]\n",
    "        y_target = self.y_targets[idx]\n",
    "        \n",
    "        return torch.tensor(ticker_idx, dtype=torch.long), \\\n",
    "               torch.tensor(X_seq, dtype=torch.float32), \\\n",
    "               torch.tensor(y_target, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 2. GRU MODEL ARCHITECTURE\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, num_entities, embed_dim, input_dim, hidden_size, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        # 1. Ticker ID Embedding Layer\n",
    "        self.id_embedding = nn.Embedding(num_entities, embed_dim)\n",
    "        \n",
    "        # Total feature size = original features + embedding dimension\n",
    "        self.total_input_dim = input_dim + embed_dim\n",
    "        \n",
    "        # 2. GRU Layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.total_input_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 3. Output Layer (Sequence-to-single value: predict one Close price)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, ticker_idx, X_seq):\n",
    "        # Generate ID embedding\n",
    "        id_embed = self.id_embedding(ticker_idx)\n",
    "        \n",
    "        # Expand embedding across the sequence length\n",
    "        id_embed = id_embed.unsqueeze(1).repeat(1, X_seq.size(1), 1)\n",
    "        \n",
    "        # Concatenate embedding with features at every timestep\n",
    "        combined_input = torch.cat((X_seq, id_embed), dim=2)\n",
    "        \n",
    "        # Pass through GRU\n",
    "        out, h_n = self.gru(combined_input)\n",
    "        \n",
    "        # Use the final hidden state of the last layer\n",
    "        final_output = self.fc(h_n[-1, :, :])\n",
    "        \n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64eee8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/2x8pccyj4l186_tbnbtkf8_40000gn/T/ipykernel_81164/2640708067.py:6: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"all_stocks_2017-01-01_to_2018-01-01.csv\")\n",
    "\n",
    "\n",
    "# Handle missing data by filling with the next valid observation (standard for time series)\n",
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abac7f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 46/46 [00:00<00:00, 125.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete. Train Loss: 0.0096, Test Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 46/46 [00:00<00:00, 184.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete. Train Loss: 0.0007, Test Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 46/46 [00:00<00:00, 204.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete. Train Loss: 0.0005, Test Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 46/46 [00:00<00:00, 211.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete. Train Loss: 0.0003, Test Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 46/46 [00:00<00:00, 215.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete. Train Loss: 0.0002, Test Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 46/46 [00:00<00:00, 184.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Complete. Train Loss: 0.0002, Test Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 46/46 [00:00<00:00, 214.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Complete. Train Loss: 0.0002, Test Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 46/46 [00:00<00:00, 217.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Complete. Train Loss: 0.0002, Test Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 46/46 [00:00<00:00, 205.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Complete. Train Loss: 0.0001, Test Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 46/46 [00:00<00:00, 201.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Complete. Train Loss: 0.0001, Test Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 46/46 [00:00<00:00, 211.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Complete. Train Loss: 0.0001, Test Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 46/46 [00:00<00:00, 210.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Complete. Train Loss: 0.0001, Test Loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 46/46 [00:00<00:00, 197.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Complete. Train Loss: 0.0001, Test Loss: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 46/46 [00:00<00:00, 173.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Complete. Train Loss: 0.0001, Test Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 46/46 [00:00<00:00, 205.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Complete. Train Loss: 0.0001, Test Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 46/46 [00:00<00:00, 196.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Complete. Train Loss: 0.0001, Test Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 46/46 [00:00<00:00, 197.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Complete. Train Loss: 0.0001, Test Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 46/46 [00:00<00:00, 201.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Complete. Train Loss: 0.0001, Test Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 46/46 [00:00<00:00, 193.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Complete. Train Loss: 0.0001, Test Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 46/46 [00:00<00:00, 213.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Complete. Train Loss: 0.0001, Test Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 46/46 [00:00<00:00, 191.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Complete. Train Loss: 0.0001, Test Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 46/46 [00:00<00:00, 165.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Complete. Train Loss: 0.0001, Test Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 46/46 [00:00<00:00, 217.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Complete. Train Loss: 0.0001, Test Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 46/46 [00:00<00:00, 219.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Complete. Train Loss: 0.0001, Test Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 46/46 [00:00<00:00, 214.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 46/46 [00:00<00:00, 214.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Complete. Train Loss: 0.0001, Test Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 46/46 [00:00<00:00, 218.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 46/46 [00:00<00:00, 222.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 46/46 [00:00<00:00, 216.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Complete. Train Loss: 0.0001, Test Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 46/46 [00:00<00:00, 173.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Complete. Train Loss: 0.0001, Test Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 46/46 [00:00<00:00, 198.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 46/46 [00:00<00:00, 204.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Complete. Train Loss: 0.0001, Test Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 46/46 [00:00<00:00, 213.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Complete. Train Loss: 0.0001, Test Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 46/46 [00:00<00:00, 220.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Complete. Train Loss: 0.0001, Test Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 46/46 [00:00<00:00, 214.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 46/46 [00:00<00:00, 214.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Complete. Train Loss: 0.0001, Test Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 46/46 [00:00<00:00, 189.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Complete. Train Loss: 0.0001, Test Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 46/46 [00:00<00:00, 172.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Complete. Train Loss: 0.0001, Test Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 46/46 [00:00<00:00, 216.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 46/46 [00:00<00:00, 203.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Complete. Train Loss: 0.0001, Test Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 46/46 [00:00<00:00, 194.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 46/46 [00:00<00:00, 189.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 46/46 [00:00<00:00, 209.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Complete. Train Loss: 0.0001, Test Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 46/46 [00:00<00:00, 218.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Complete. Train Loss: 0.0001, Test Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 46/46 [00:00<00:00, 216.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Complete. Train Loss: 0.0001, Test Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 46/46 [00:00<00:00, 193.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Complete. Train Loss: 0.0001, Test Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 46/46 [00:00<00:00, 200.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Complete. Train Loss: 0.0001, Test Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 46/46 [00:00<00:00, 185.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Complete. Train Loss: 0.0001, Test Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 46/46 [00:00<00:00, 209.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Complete. Train Loss: 0.0000, Test Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 46/46 [00:00<00:00, 217.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Complete. Train Loss: 0.0000, Test Loss: 0.0013\n",
      "\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SEQUENCE_LENGTH = 10     # Look back 10 days\n",
    "TARGET_COLUMN = 'Close'  # Predict the next day's closing price\n",
    "EMBEDDING_DIM = 8        # Size of the Ticker ID embedding vector\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "# ---------------------\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "\n",
    "\n",
    "# Identify features and create Ticker mapping\n",
    "ID_COL = 'Name'\n",
    "FEATURE_COLS = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "UNIQUE_TICKERS = df[ID_COL].unique()\n",
    "NUM_ENTITIES = len(UNIQUE_TICKERS)\n",
    "\n",
    "# Create a mapping from actual Ticker to a zero-indexed integer\n",
    "id_mapping = {id_val: idx for idx, id_val in enumerate(UNIQUE_TICKERS)}\n",
    "\n",
    "\n",
    "# Normalize Features (Crucial for deep learning)\n",
    "scaler = MinMaxScaler()\n",
    "# Fit and transform the features across the entire dataset\n",
    "df[FEATURE_COLS] = scaler.fit_transform(df[FEATURE_COLS])\n",
    "\n",
    "\n",
    "# Split data by Ticker ID (to ensure sequences from one company don't leak into another split)\n",
    "train_tickers, test_tickers = train_test_split(UNIQUE_TICKERS, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df[ID_COL].isin(train_tickers)].copy()\n",
    "test_df = df[df[ID_COL].isin(test_tickers)].copy()\n",
    "\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = StockTimeseriesDataset(train_df, SEQUENCE_LENGTH, TARGET_COLUMN, FEATURE_COLS, ID_COL, id_mapping)\n",
    "test_dataset = StockTimeseriesDataset(test_df, SEQUENCE_LENGTH, TARGET_COLUMN, FEATURE_COLS, ID_COL, id_mapping)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "INPUT_DIM = len(FEATURE_COLS)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GRUModel(\n",
    "    num_entities=NUM_ENTITIES,\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    input_dim=INPUT_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# 3. TRAINING LOOP\n",
    "print(f\"\\nStarting training on {device}...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for ticker_idx, X_seq, y_target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        \n",
    "        # Move data to device and adjust target shape\n",
    "        ticker_idx, X_seq, y_target = ticker_idx.to(device), X_seq.to(device), y_target.to(device).unsqueeze(1)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(ticker_idx, X_seq)\n",
    "        loss = criterion(output, y_target)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # 4. EVALUATION (Testing)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for ticker_idx, X_seq, y_target in test_loader:\n",
    "            ticker_idx, X_seq, y_target = ticker_idx.to(device), X_seq.to(device), y_target.to(device).unsqueeze(1)\n",
    "            \n",
    "            output = model(ticker_idx, X_seq)\n",
    "            loss = criterion(output, y_target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete. Train Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e336ebed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.2826e-01, -7.8797e-01, -6.4987e-01,  6.1657e-01,  5.2846e-01,\n",
       "          1.5343e-01, -2.1592e+00, -2.8042e-01],\n",
       "        [ 1.6847e+00,  3.0657e-01, -1.5596e+00,  2.0460e-02, -1.7752e+00,\n",
       "          2.1215e+00,  1.3236e+00, -2.3348e-01],\n",
       "        [ 2.7797e-01,  5.8675e-01, -9.0585e-01, -5.5395e-02, -3.4458e-01,\n",
       "          9.3455e-01, -3.5398e-01, -1.8477e+00],\n",
       "        [-3.7466e-01,  1.1362e+00,  6.7229e-01,  1.1681e+00, -9.0135e-01,\n",
       "          4.4223e-01,  1.3015e+00,  3.3303e-01],\n",
       "        [-1.2875e+00,  1.1090e+00, -8.5827e-02,  3.5956e-01,  1.6598e+00,\n",
       "         -9.6496e-01,  1.4201e+00, -1.7681e-01],\n",
       "        [-2.3735e+00, -2.5646e-01,  4.0951e-01, -2.2480e-01,  1.8452e-01,\n",
       "         -6.4926e-01, -1.2565e+00,  1.4070e+00],\n",
       "        [ 1.9081e-01,  1.0518e+00, -5.1251e-01,  6.7694e-01,  1.0424e+00,\n",
       "         -1.3231e+00, -9.7702e-01,  6.8235e-01],\n",
       "        [ 9.9218e-01,  6.7542e-01, -6.7058e-01, -6.7242e-01, -4.7832e-01,\n",
       "         -1.3384e+00,  2.1459e+00,  3.3527e-01],\n",
       "        [ 6.2146e-01, -1.6979e-01,  2.0751e+00, -1.1310e+00,  7.1039e-01,\n",
       "          9.9525e-01, -1.5828e+00, -7.3733e-01],\n",
       "        [ 3.2517e+00, -5.5584e-01,  4.6742e-01, -6.9104e-01,  8.9499e-02,\n",
       "          3.0888e-01,  5.8245e-01, -9.0815e-01],\n",
       "        [ 1.5370e+00, -1.6615e-01, -2.6675e-01, -5.6383e-01,  4.2816e-01,\n",
       "         -3.6241e-01,  9.7443e-03, -1.1380e-01],\n",
       "        [-3.6510e-02,  6.4972e-01,  8.1921e-01,  1.7225e+00, -8.0067e-01,\n",
       "          5.1468e-02, -2.6579e-01, -9.5236e-01],\n",
       "        [-7.0087e-02,  9.1010e-01,  1.8445e-01,  4.2452e-01,  2.6979e-01,\n",
       "          5.6945e-01, -2.6774e-01, -8.1401e-01],\n",
       "        [-2.5413e+00,  5.4997e-01, -1.6127e+00,  1.8683e+00,  8.9374e-01,\n",
       "         -4.0136e-01, -1.3885e-01, -1.4110e+00],\n",
       "        [-4.7064e-01, -1.2532e+00,  1.7279e+00,  3.1078e-01, -1.2924e+00,\n",
       "         -1.8421e-01, -7.9150e-01, -3.0383e-01],\n",
       "        [ 2.0419e-01,  1.4702e+00,  3.4065e-02, -1.6062e+00, -1.6980e+00,\n",
       "         -1.1497e+00,  7.4102e-01,  6.5506e-01],\n",
       "        [ 1.0585e+00, -1.2382e+00, -1.4531e+00,  3.1227e-01,  4.1204e-01,\n",
       "          7.8465e-01, -7.2616e-01,  4.2632e-01],\n",
       "        [ 4.1043e-01, -1.4556e-01, -4.5861e-01, -3.4598e-01,  1.0285e+00,\n",
       "         -1.3235e+00,  1.4058e+00, -4.8631e-01],\n",
       "        [ 5.4502e-01, -6.0911e-01,  1.3072e+00,  1.1792e+00,  5.6982e-01,\n",
       "         -2.9772e-01, -1.3148e+00,  6.8026e-01],\n",
       "        [ 5.3550e-01,  4.1597e-01,  2.2686e+00,  1.0437e+00, -1.5089e+00,\n",
       "          2.3661e+00,  3.5716e-01, -5.4782e-01],\n",
       "        [ 8.1151e-01,  5.5360e-01,  1.9322e+00, -9.5674e-01,  4.9743e-01,\n",
       "          1.5123e+00,  4.9429e-01,  4.2344e-01],\n",
       "        [-6.3031e-02, -1.6424e+00, -1.3763e+00,  5.4876e-01, -2.8247e-01,\n",
       "         -1.9034e+00, -1.7659e+00, -2.0631e-01],\n",
       "        [ 4.2297e-01,  9.2462e-01,  1.3731e+00, -1.4350e-01,  6.3530e-01,\n",
       "          1.2019e+00,  6.9332e-01, -1.7546e+00],\n",
       "        [-1.9249e-01,  8.1580e-01,  1.8742e-01,  8.8659e-01, -2.4546e-01,\n",
       "          1.2371e-03, -2.4727e+00,  1.3345e+00],\n",
       "        [ 1.0872e+00,  6.1276e-01, -1.4445e+00, -9.7837e-01, -7.5527e-01,\n",
       "          4.1107e-01, -1.0268e+00, -2.5041e-01],\n",
       "        [ 1.3563e-03, -7.0430e-01,  3.6573e-01,  1.0258e+00,  3.3588e-02,\n",
       "          5.8338e-01,  1.0109e+00, -5.0968e-01],\n",
       "        [-1.4426e+00, -6.1216e-01,  1.0149e-01, -7.5561e-01, -2.0068e-01,\n",
       "         -1.1184e+00, -3.1545e-01, -1.0319e+00],\n",
       "        [ 8.8470e-01,  2.4711e-01,  3.0056e+00, -1.3204e+00,  1.6689e-01,\n",
       "         -1.6695e-01,  3.7660e-02, -2.6941e-01],\n",
       "        [ 8.1817e-01,  6.5394e-01,  1.3421e+00, -1.8050e-01, -1.4422e+00,\n",
       "         -1.2858e-02, -5.6388e-01,  1.7447e+00],\n",
       "        [-2.8947e-01,  1.6094e+00, -1.4389e+00, -1.0506e+00, -2.5095e-01,\n",
       "         -7.7128e-01,  5.9506e-01,  1.5401e-01],\n",
       "        [ 1.3477e+00, -1.1426e+00, -3.5688e-01,  9.0799e-01,  1.7033e-01,\n",
       "         -5.6158e-01,  1.6844e+00,  1.3201e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.id_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41ba9435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0091],\n",
       "        [0.0090],\n",
       "        [0.0089],\n",
       "        [0.0089],\n",
       "        [0.0090],\n",
       "        [0.0091],\n",
       "        [0.0092],\n",
       "        [0.0093],\n",
       "        [0.0093],\n",
       "        [0.0093],\n",
       "        [0.0094],\n",
       "        [0.0095],\n",
       "        [0.0095],\n",
       "        [0.0096],\n",
       "        [0.0095],\n",
       "        [0.0095],\n",
       "        [0.0095],\n",
       "        [0.0095],\n",
       "        [0.0095],\n",
       "        [0.0096],\n",
       "        [0.0096],\n",
       "        [0.0097],\n",
       "        [0.0098]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57246041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/2x8pccyj4l186_tbnbtkf8_40000gn/T/ipykernel_71833/910655993.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# ... import other libraries (tqdm, numpy, etc.)\n",
    "\n",
    "# --- Configuration (Same as before) ---\n",
    "SEQUENCE_LENGTH = 10 \n",
    "# ...\n",
    "\n",
    "# 1. DATA PREPARATION (Filtering and Scaling)\n",
    "df = pd.read_csv(\"all_stocks_2017-01-01_to_2018-01-01.csv\")\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "# --- Select and Filter for ONLY ONE Stock (e.g., AAPL) ---\n",
    "SINGLE_STOCK_NAME = 'AAPL'\n",
    "single_stock_df = df[df['Name'] == SINGLE_STOCK_NAME].copy()\n",
    "\n",
    "FEATURE_COLS = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "TARGET_COLUMN = 'Close'\n",
    "\n",
    "# Normalize Features (Fit only on this stock's data)\n",
    "scaler = MinMaxScaler()\n",
    "single_stock_df[FEATURE_COLS] = scaler.fit_transform(single_stock_df[FEATURE_COLS])\n",
    "\n",
    "# 2. Dataset Creation (No ID/Mapping needed)\n",
    "class SingleStockDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, seq_len, target_col, feature_cols):\n",
    "        # ... logic to create X and y sequences (similar to previous class, but without ID)\n",
    "        self.X_sequences = []\n",
    "        self.y_targets = []\n",
    "        \n",
    "        features = df[feature_cols].values\n",
    "        target = df[target_col].values\n",
    "        \n",
    "        for i in range(len(df) - seq_len):\n",
    "            self.X_sequences.append(features[i : i + seq_len])\n",
    "            self.y_targets.append(target[i + seq_len])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X_sequences[idx], dtype=torch.float32), \\\n",
    "               torch.tensor(self.y_targets[idx], dtype=torch.float32)\n",
    "\n",
    "# Splitting this single stock data (e.g., 80% train, 20% test)\n",
    "train_split = int(0.8 * len(single_stock_df))\n",
    "train_df = single_stock_df.iloc[:train_split]\n",
    "test_df = single_stock_df.iloc[train_split:]\n",
    "\n",
    "train_dataset_single = SingleStockDataset(train_df, SEQUENCE_LENGTH, TARGET_COLUMN, FEATURE_COLS)\n",
    "test_dataset_single = SingleStockDataset(test_df, SEQUENCE_LENGTH, TARGET_COLUMN, FEATURE_COLS)\n",
    "\n",
    "train_loader_single = DataLoader(train_dataset_single, batch_size=64, shuffle=True)\n",
    "test_loader_single = DataLoader(test_dataset_single, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# 3. SINGLE GRU MODEL Architecture\n",
    "class SingleGRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers):\n",
    "        super(SingleGRUModel, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim, # No embedding added here\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        # X_seq: (batch_size, seq_len, input_dim)\n",
    "        out, h_n = self.gru(X_seq)\n",
    "        final_output = self.fc(h_n[-1, :, :])\n",
    "        return final_output\n",
    "\n",
    "# 4. Training Loop (Similar to the shared model loop, but without the 'ID' handling)\n",
    "# ... Initialize SingleGRUModel and train using train_loader_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76b1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.150627</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072392</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>0.017471</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.083371</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>0.016523</td>\n",
       "      <td>0.030064</td>\n",
       "      <td>0.028453</td>\n",
       "      <td>0.031291</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>0.036250</td>\n",
       "      <td>0.050928</td>\n",
       "      <td>0.052912</td>\n",
       "      <td>0.049172</td>\n",
       "      <td>0.199424</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>2017-10-11</td>\n",
       "      <td>0.677289</td>\n",
       "      <td>0.667817</td>\n",
       "      <td>0.682030</td>\n",
       "      <td>0.671026</td>\n",
       "      <td>0.029390</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>0.683696</td>\n",
       "      <td>0.674224</td>\n",
       "      <td>0.681697</td>\n",
       "      <td>0.661921</td>\n",
       "      <td>0.021421</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2017-10-13</td>\n",
       "      <td>0.690103</td>\n",
       "      <td>0.672745</td>\n",
       "      <td>0.693012</td>\n",
       "      <td>0.678311</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>0.709830</td>\n",
       "      <td>0.717431</td>\n",
       "      <td>0.713644</td>\n",
       "      <td>0.726159</td>\n",
       "      <td>0.103052</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2017-10-17</td>\n",
       "      <td>0.741528</td>\n",
       "      <td>0.731723</td>\n",
       "      <td>0.739933</td>\n",
       "      <td>0.735927</td>\n",
       "      <td>0.050742</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Open      High       Low     Close    Volume  Name\n",
       "502  2017-01-03  0.000000  0.000000  0.000000  0.002152  0.150627  AAPL\n",
       "503  2017-01-04  0.000843  0.002957  0.016473  0.000000  0.072392  AAPL\n",
       "504  2017-01-05  0.002023  0.008707  0.017471  0.009768  0.083371  AAPL\n",
       "505  2017-01-06  0.016523  0.030064  0.028453  0.031291  0.180947  AAPL\n",
       "506  2017-01-09  0.036250  0.050928  0.052912  0.049172  0.199424  AAPL\n",
       "..          ...       ...       ...       ...       ...       ...   ...\n",
       "697  2017-10-11  0.677289  0.667817  0.682030  0.671026  0.029390  AAPL\n",
       "698  2017-10-12  0.683696  0.674224  0.681697  0.661921  0.021421  AAPL\n",
       "699  2017-10-13  0.690103  0.672745  0.693012  0.678311  0.024169  AAPL\n",
       "700  2017-10-16  0.709830  0.717431  0.713644  0.726159  0.103052  AAPL\n",
       "701  2017-10-17  0.741528  0.731723  0.739933  0.735927  0.050742  AAPL\n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0b638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
